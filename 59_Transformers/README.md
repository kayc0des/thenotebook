# Transformers

Transformers are a type of neural network architecture that are used for natural language processing tasks such as language modeling, text generation, and machine translation.

## History

Transformers were first introduced in 2017 by Vaswani et al. in their paper "Attention is All You Need". They quickly became the state-of-the-art model for natural language processing tasks due to their ability to handle long-range dependencies and process sequential data in parallel.

## Architecture

The transformer architecture consists of an encoder and a decoder. The encoder takes the input sequence and produces a sequence of hidden states, while the decoder takes the output sequence and produces a sequence of hidden states.

## Attention Mechanism

The attention mechanism is a key component of the transformer architecture. It allows the model to focus on the most relevant parts of the input sequence when producing the output sequence.

## Self-Attention

