{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK2LIlXPuyTW"
      },
      "source": [
        "# Tokenization Practicals\n",
        "\n",
        "Tokenization in Natural Language Processing (NLP) is like breaking down a sentence into smaller pieces, called \"tokens.\" Imagine you have a sentence like \"I love pizza.\" When you tokenize it, you might break it down into individual words: [\"I\", \"love\", \"pizza\"].\n",
        "\n",
        "This process helps computers understand and analyze text more easily. Instead of dealing with one long string of text, they work with these smaller chunks, which can be words, phrases, or even characters. Tokenization is the first step in many NLP tasks, allowing machines to process and interpret language more effectively!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qFENrnfl8aM",
        "outputId": "71688117-00ff-48b0-b68f-b3550f8303f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "# Website -> https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AGc-WG8GmEw-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-qkHlm7xPVL",
        "outputId": "5f460729-7ea9-4092-83ae-85b382f292dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. Through NLP, machines can understand, interpret, and generate human languages, making it possible for applications like chatbots, voice assistants, and\n",
            "language translation tools to function. Techniques like tokenization, stemming, and lemmatization help break down text into manageable units for processing. Word embeddings, on the other hand, provide meaningful numeric representations of words, enabling models to capture relationships between words based on their usage. As NLP continues to evolve, its impact on technology and everyday life becomes increasingly significant.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "corpus = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. Through NLP, machines can understand, interpret, and generate human languages, making it possible for applications like chatbots, voice assistants, and\n",
        "language translation tools to function. Techniques like tokenization, stemming, and lemmatization help break down text into manageable units for processing. Word embeddings, on the other hand, provide meaningful numeric representations of words, enabling models to capture relationships between words based on their usage. As NLP continues to evolve, its impact on technology and everyday life becomes increasingly significant.\n",
        "\"\"\"\n",
        "\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPuzBQN4mTH3",
        "outputId": "11b87235-cdbd-4192-beb4-0e1dbc65c91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document: Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.\n",
            "Document: Through NLP, machines can understand, interpret, and generate human languages, making it possible for applications like chatbots, voice assistants, and\n",
            "language translation tools to function.\n",
            "Document: Techniques like tokenization, stemming, and lemmatization help break down text into manageable units for processing.\n",
            "Document: Word embeddings, on the other hand, provide meaningful numeric representations of words, enabling models to capture relationships between words based on their usage.\n",
            "Document: As NLP continues to evolve, its impact on technology and everyday life becomes increasingly significant.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Tokenize corpus to sentences\n",
        "\n",
        "nltk.download('punkt')\n",
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "for doc in documents:\n",
        "    print(f'Document: {doc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTvVZfgT0uWc",
        "outputId": "10be9217-08b3-4673-d515-9efac9fcfa32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'Through', 'NLP', ',', 'machines', 'can', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'languages', ',', 'making', 'it', 'possible', 'for', 'applications', 'like', 'chatbots', ',', 'voice', 'assistants', ',', 'and', 'language', 'translation', 'tools', 'to', 'function', '.', 'Techniques', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'help', 'break', 'down', 'text', 'into', 'manageable', 'units', 'for', 'processing', '.', 'Word', 'embeddings', ',', 'on', 'the', 'other', 'hand', ',', 'provide', 'meaningful', 'numeric', 'representations', 'of', 'words', ',', 'enabling', 'models', 'to', 'capture', 'relationships', 'between', 'words', 'based', 'on', 'their', 'usage', '.', 'As', 'NLP', 'continues', 'to', 'evolve', ',', 'its', 'impact', 'on', 'technology', 'and', 'everyday', 'life', 'becomes', 'increasingly', 'significant', '.']\n",
            "[['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.'], ['Through', 'NLP', ',', 'machines', 'can', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'languages', ',', 'making', 'it', 'possible', 'for', 'applications', 'like', 'chatbots', ',', 'voice', 'assistants', ',', 'and', 'language', 'translation', 'tools', 'to', 'function', '.'], ['Techniques', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'help', 'break', 'down', 'text', 'into', 'manageable', 'units', 'for', 'processing', '.'], ['Word', 'embeddings', ',', 'on', 'the', 'other', 'hand', ',', 'provide', 'meaningful', 'numeric', 'representations', 'of', 'words', ',', 'enabling', 'models', 'to', 'capture', 'relationships', 'between', 'words', 'based', 'on', 'their', 'usage', '.'], ['As', 'NLP', 'continues', 'to', 'evolve', ',', 'its', 'impact', 'on', 'technology', 'and', 'everyday', 'life', 'becomes', 'increasingly', 'significant', '.']]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize paragraph -> words\n",
        "words = word_tokenize(corpus)\n",
        "print(words)\n",
        "\n",
        "# Sentence words -> words\n",
        "words_list = []\n",
        "\n",
        "for doc in documents:\n",
        "    words_list.append(word_tokenize(doc))\n",
        "\n",
        "print(words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IHv1mCk2MCN",
        "outputId": "39d175a9-f6b5-4424-aa45-4ba8ac972da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'Through', 'NLP', ',', 'machines', 'can', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'languages', ',', 'making', 'it', 'possible', 'for', 'applications', 'like', 'chatbots', ',', 'voice', 'assistants', ',', 'and', 'language', 'translation', 'tools', 'to', 'function', '.', 'Techniques', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'help', 'break', 'down', 'text', 'into', 'manageable', 'units', 'for', 'processing', '.', 'Word', 'embeddings', ',', 'on', 'the', 'other', 'hand', ',', 'provide', 'meaningful', 'numeric', 'representations', 'of', 'words', ',', 'enabling', 'models', 'to', 'capture', 'relationships', 'between', 'words', 'based', 'on', 'their', 'usage', '.', 'As', 'NLP', 'continues', 'to', 'evolve', ',', 'its', 'impact', 'on', 'technology', 'and', 'everyday', 'life', 'becomes', 'increasingly', 'significant', '.']\n"
          ]
        }
      ],
      "source": [
        "# Using wordpunct_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "word_punct = wordpunct_tokenize(corpus)\n",
        "\n",
        "print(word_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c62FoBCD2-OM",
        "outputId": "766ce386-6f22-4e43-84b9-125df0c77168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language.', 'Through', 'NLP', ',', 'machines', 'can', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'languages', ',', 'making', 'it', 'possible', 'for', 'applications', 'like', 'chatbots', ',', 'voice', 'assistants', ',', 'and', 'language', 'translation', 'tools', 'to', 'function.', 'Techniques', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'help', 'break', 'down', 'text', 'into', 'manageable', 'units', 'for', 'processing.', 'Word', 'embeddings', ',', 'on', 'the', 'other', 'hand', ',', 'provide', 'meaningful', 'numeric', 'representations', 'of', 'words', ',', 'enabling', 'models', 'to', 'capture', 'relationships', 'between', 'words', 'based', 'on', 'their', 'usage.', 'As', 'NLP', 'continues', 'to', 'evolve', ',', 'its', 'impact', 'on', 'technology', 'and', 'everyday', 'life', 'becomes', 'increasingly', 'significant', '.']\n"
          ]
        }
      ],
      "source": [
        "# TreebankWordTokenizer - All full stops except the last one in the corpus are not treated as separate words.\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "words = tokenizer.tokenize(corpus)\n",
        "\n",
        "print(words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
