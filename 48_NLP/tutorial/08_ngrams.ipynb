{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Definition: A contiguous sequence of items from a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad' 'dolor' 'dolore' 'in' 'incididunt ut' 'ipsum' 'ipsum dolor' 'irure'\n",
      " 'irure dolor' 'labore' 'labore et' 'laboris' 'laboris nisi' 'laborum'\n",
      " 'lorem' 'lorem ipsum' 'magna' 'magna aliqua' 'minim' 'minim veniam'\n",
      " 'mollit' 'mollit anim' 'nisi' 'nisi ut' 'non' 'non proident' 'nostrud'\n",
      " 'nostrud exercitation' 'nulla' 'nulla pariatur' 'occaecat'\n",
      " 'occaecat cupidatat' 'officia' 'ut' 'ut enim' 'ut labore' 'velit'\n",
      " 'velit esse' 'veniam' 'veniam quis']\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to create bag of words model with uni and bi-grams\n",
    "\n",
    "# Import the CountVectorizer class from sklearn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents for demonstration\n",
    "corpus = [\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "    \"Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
    "    \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\",\n",
    "    \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\",\n",
    "    \"Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance with specific parameters\n",
    "# lowercase=True: Convert all characters to lowercase before tokenizing\n",
    "# ngram_range=(1, 2): Create both unigrams (single words) and bigrams (pairs of consecutive words)\n",
    "vectorizer = CountVectorizer(max_features=40, lowercase=True, ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the documents into a matrix of token counts\n",
    "# fit_transform() method does both fitting (learning the vocabulary) and transforming (creating the document-term matrix) in one step\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the sparse matrix to a dense numpy array for easier viewing\n",
    "# Note: For large datasets, it's often better to work with the sparse matrix directly to save memory\n",
    "X.toarray()\n",
    "\n",
    "# Get the feature names (words and n-grams) that correspond to the columns in the resulting matrix\n",
    "# These represent the vocabulary learned by the vectorizer\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
